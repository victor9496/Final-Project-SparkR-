---
title: "Part-II Writeup"
author: "Team: SparkR"
date: "2017/12/7"
output: pdf_document
---

```{r setup, include=FALSE}
#load the necessart packages
knitr::opts_chunk$set(echo = FALSE, warning=FALSE, message=FALSE)
suppressMessages(library(ggplot2))
suppressMessages(library(dplyr))
suppressMessages(library(reshape2))
suppressMessages(library(tidyr))
suppressMessages(library(gridExtra))
suppressMessages(library(grid))
suppressMessages(library(knitr))
suppressMessages(library(xgboost))
suppressMessages(library(BayesTree))
suppressMessages(library(randomForest))
suppressMessages(library(BAS))
suppressMessages(library(gbm))
suppressMessages(library(GGally))
```

#Introduction

Late 18th century Paris has witnessed countless number of auctions on fine arts and paintings from countries across Europe. Our project utilizes this dataset from auction in 18th century Paris with details of paintings, auctions, dealers and other third parties involved. 

We hope to build a model to predict the log price fetched at auction. Since our dataset contains 59 variables, we would want to perform variable selection to build a parsimonious model with good performance in critiria such as RMSE, etc. For this part, since we are not restricted to OLS, we did more exploration into models we have learned this semester, e.g. random forest, boosting, bayesian additive regression, tree.

In addition to that, some variables have a lot of missing values and typos. We would need to perform some sort of missing imputation, e.g. assume MCAR and use MICE package. For typos, we would recode them to appropriate values. During predicting with validation data, we found some new levels in some variables, e.g. material. So we modified our data cleaning process so our dataframe could be used for various models. 

From the correlation martices, we find some highly-correlated variables, so we decide to not include some of the variables in our model to aviod multicollinearity. Algorithm such as gradient boosting handles highly correlated predictors very well, so models will be robust.

#Exploratory data analysis

```{r echo=FALSE,warning=FALSE}
#load the necessary data sets
load("paintings_test.Rdata")
load("paintings_train.Rdata")
```


```{r echo=FALSE,warning=FALSE}
#function to clean the training and testing data sets
clean.new = function(df, NA.omit=F){
   df = df %>% 
     select(-c(sale, price, count, subject, winningbidder, authorstandard, authorstyle, author, Height_in, Width_in, Surface_Rect, Diam_in, winningbiddertype, Surface_Rnd,material,materialCat, Interm)) %>%
     mutate(lot = as.numeric(lot)) %>%
     mutate(mat=as.character(mat))%>%
     mutate(mat_recode = ifelse(mat %in% c("a", "bc", "c"), "metal",
                          ifelse(mat %in% c("al", "ar", "m"), "other",
                                ifelse(mat %in% c("co", "bt", "t"), "canvas",
                                       ifelse(mat %in% c("p", "ca"), "paper",
                                              ifelse(mat %in% c("b"), "wood",
                                                     ifelse(mat %in% c("o", "e", "v"), "other",
                                                            ifelse(mat %in% c("n/a", ""), "na",
                                                                   "other")))))))) %>%
     mutate(school_pntg=as.character(school_pntg))%>%
     mutate(school_pntg_recode = ifelse(school_pntg %in% c("A", "G", "S"), "other",school_pntg)) %>%
     mutate(origin_cat=ifelse(origin_cat %in% c("X","S"),"O",origin_cat))%>%
     mutate(SurfaceNA = ifelse(is.na(df$Surface), 1, 0))%>%
     mutate(nfigures=as.numeric(nfigures))%>%
     mutate(Surface=log(Surface+1))%>%
     mutate(endbuyer=as.character(endbuyer))%>%
     mutate(endbuyer = ifelse(endbuyer == "", "NA", endbuyer)) %>% 
     mutate(Shape=as.character(Shape))%>%
   mutate(shape_recode = ifelse(Shape == "", "Other",
                               ifelse(Shape == "ovale", "oval",
                                      ifelse(Shape == "ronde", "round",
                                             ifelse(Shape %in% c("octogon","octagon"), "Other", Shape)))))%>%
     select(-c(mat,school_pntg, Shape))%>%
     mutate(type_intermed=as.character(type_intermed))%>%
     mutate(type_intermed=ifelse(type_intermed == "EB","B",type_intermed))
   
   
   factornames=colnames(df)[!colnames(df) %in% c("logprice","lot","position","Surface","nfigures")] 
   
   
   df[factornames] = lapply(df[factornames], factor)

   df$Surface[is.na(df$Surface)] = 0
  
  
   if(NA.omit==T){
      df = na.omit(df)
   }

  return(df) 
}

#function to calculate rmse
rmse <- function(y, ypred) {
rmse <- sqrt(mean((y - ypred)^2))
return(rmse)
}
```


```{r echo=FALSE,warning=FALSE}
#clean the training and testing data sets
train = clean.new(paintings_train) %>% select(-lot)
test = clean.new(paintings_test)%>% select(-lot)
```

For this part, we intially have found some interaction between predictors from graph perspective; however, in our further model selection process and trials and error.

First, we choose to plot boxplots for the variables that are highly correlated with response `logprice`

```{r echo=FALSE, warning=FALSE}
train_sub1 = train %>% select(logprice, dealer, endbuyer, type_intermed, lrgfont)


df1 = train %>% 
  select(logprice, dealer) 

df2 = train %>% 
  select(logprice, endbuyer)

df3 = train %>% 
  select(logprice, type_intermed)

df4 = train %>% 
  select(logprice, lrgfont) 

# df1.final = rbind(df1,df2,df3, df4)
# 
# ggplot(train, aes(x=type, y= logprice)) +
#   facet_wrap(~type) +
#   geom_boxplot()
```

```{r echo=FALSE, warning=FALSE}
g1 = ggplot(df1, aes(x=dealer, y= logprice)) +
  geom_boxplot(color="#999999",  alpha=0.8)
g2 = ggplot(df2, aes(x=endbuyer, y= logprice)) +
  geom_boxplot(color="#E69F00",  alpha=0.8)
g3 = ggplot(df3, aes(x=type_intermed, y= logprice)) +
  geom_boxplot(color="#56B4E9",  alpha=0.8)
g4 = ggplot(df4, aes(x=lrgfont, y= logprice)) +
  geom_boxplot(color="purple",  alpha=0.8)

grid.arrange(g1, g2,g3,g4, ncol=2, top=textGrob("Boxplot for 4 most 'significant' variables", gp=gpar(fontsize=12,font=8)))
```


As we can see, for these four factor variables, each level has differernt means and distributions across, for examle, doing business with dealer R may not be a good deal, since they tend to have higher price, but it could also be the case that they tend to sell higher value painting; also, collectors tend bid higher price compared with other end buyers; from type of inttype of intermediary perspective, experts tend to have higher price in mind,(becasue they know the true intrinsic value?), lastly, if dealer have more information about the paintings, the final price will be higher as well. Therefore, it is likely human factors(especially type of dealer) play an important role on deciding the final bidding price, and we will expect they will "survive" after model selection process.


Second, we try to find  some interaction effect between predictors using tree models.

```{r echo=FALSE, warning=FALSE}
# library(tree)
# tree.vote =tree(logprice ~ ., data= train)
# summary(tree.vote)
# plot(tree.vote)
# text(tree.vote, cex=.75)
# 
# #Pruning
# set.seed(2)
# cv.vote = cv.tree(tree.vote, FUN=prune.tree)
# plot(cv.vote$size,cv.vote$dev, type="b")
# prune.vote = prune.tree(tree.vote ,best =11)
# plot(prune.vote)
# text(prune.vote, cex=.75)
suppressMessages(library(RColorBrewer))


df5 = paintings_train %>% 
  select(price, year, school_pntg) %>% 
  mutate(year = as.character(year)) %>% 
  mutate(year = as.numeric(year)) %>%
  mutate(price = as.numeric(price)) %>% 
  filter(!is.na(price)) %>% 
  group_by(year, school_pntg) %>% 
  summarize(average = sum(price)/n())%>% 
  group_by(school_pntg)


ggplot(data = df5, aes(x = year, y= average, col = school_pntg)) +
  geom_line(size = 1.5) +
  scale_color_brewer(type = 'div', palette = 'Spectral')+
  xlab("Year of sale")+
  ylab("Average price")+
  ggtitle("Average price of differnt school of paintings on each year") +
  labs(col = "School of Paintings") +
  theme_bw()

```

Since when plotting average logprice, all the line lie between each other, we decide to plot price in original scale instead. As you can see from the plot, Dutch/Flemish and French school of paintings tend to have a higher average price in the year between 1765 and 1778, with the maximum across the group, Dutch/Flemish has the average price over 600 dollars for the paintings in the year around 1770(possibly becasue of the decease of some well-known artists);Even though Italian has lowever in the earlier year, they do have higher price in post-1777 era. With all these observations, it suggests there will be "some" interaction effect between  `school_pntg` and `year` that affect the overall price of paintings.

Third, we dive into predictors transformation:

```{r echo=FALSE}


#copy the training data

plot_surface=paintings_train%>%

  filter(!is.na(Surface))%>%

  select(logprice,Surface)



#before filtering

plot_wo_filter = ggplot(data = plot_surface, aes(x = Surface, y = logprice)) + 

geom_point(alpha= 0.3, size = 0.8) + 
  labs(x = expression(paste("Surface", " (", inches^2, ")")), y = "log(price)",

       title = "log(price) vs. Surface (before filtering)") +theme(plot.title = element_text(hjust = 0.5))



#filter for outliers and zeroes

plot_surface=plot_surface%>%

  filter(!is.na(Surface))%>%

  filter(Surface<8000)%>%

  filter(Surface>0)



#after filtering

plot_w_filter = ggplot(data = plot_surface, aes(x = Surface, y = logprice)) + 

geom_point(alpha= 0.3, size = 0.8) + 

  labs(x = expression(paste("Surface", " (", inches^2, ")")), y = "log(price)",

       title = "log(price) vs. Surface (after filtering)") +theme(plot.title = element_text(hjust = 0.5))



#sqrt transformation

plot_sqrt_trans=ggplot(data = plot_surface, aes(x = sqrt(Surface), y = logprice)) + 

geom_point(alpha= 0.3, size = 0.8) + 
  labs(x = expression(paste(sqrt(Surface), " (", inches^2, ")")), y = "log(price)",

       title = "log(price) vs. sqrt(Surface) ") +theme(plot.title = element_text(hjust = 0.5))



#log transformation

plot_log_trans=ggplot(data = plot_surface, aes(x = log(Surface), y = logprice)) + 

geom_point(alpha= 0.3, size = 0.8) + 
  labs(x = expression(paste("log(Surface)", " (", inches^2, ")")), y = "log(price)",

       title = "log(price) vs. log(Surface) ")+theme(plot.title = element_text(hjust = 0.5))



#plotting

grid.arrange(plot_wo_filter,plot_w_filter,plot_sqrt_trans,plot_log_trans,ncol=2)

```


At a first galance, we do not observe any significant linear relationship between the response: "log(price)" and the predictor: "Surface". After we remove some outliers and zeroes(0 surface is not meaningful), we do not achieve much improvement. Therefore, we decide to use transformations on "Surface". First, we take the squre root on "Surface". As we can see, the points are less concentrated and more normally distributed after the tansformation. When using log transformation, we have the distribution of log(Surface) very close to normal distribution. Under log transformation, we still cannot see any obvious linear relationship between the response and the predictor. However, when we perform variable selection under Bayesian information criterion, we would have "Surface", log transformed, included in the final model with a positive coefficient and a relatively small p-value. By intuition, the inclusion of Surface in the model makes sense because paintings with larger surface would probably correpond to more amount of work and more painting materials used and therefore higher prices.

```{r echo=FALSE, warning=FALSE}
train.copy = paintings_train %>% 
  select(lot,position,Surface,logprice,nfigures) %>% 
  mutate(lot = as.numeric(lot))
ggpairs(train.copy)
```

In our trainining data set, we also observed some predictors which could posssibly be highly corelated with each other. For example, "Surface" could possibly be related to "nfigures" since paintings with more figures tend to be larger than those with less figures. Moreover, "lot" could also be related with "position". We then use ggpairs() to check whether these collinearity exist. As the correlation matrix suggests, "lot" and "position" are highly corellated, therefore, we can drop one of them from our data frame. On the other hand, "Surface" and "nfigures" are not as highly corralated as we prceived. Therefore, we would like to include both predictors in our model. Moreover, position have some wrong data that are greater than 1.

#Discussion of preliminary model Part I

The overall performance for test data with our PartI model is somewhat misguiding, with the RMSE is 1273, and the coverage is about 93%, the overall performance is masked by the fact that the problem of overfitting(only add predictors that match data behevior in testset), since compared with RMSE in the test data, RMSE in the training set is about 2000; therefore, in order to robustfully predict validation data, we need to come up with a different predictors(or models) in order to corporate differenct instances of behaviors in each data set. Futher development could be use the predictors selected by step function with AIC critiria as well as BMA that can combine different models and lower the variances.


#Development of the final model


 * Data Set Modification:  

As mentioned in the our previous exploratory data analysis part, although we had relatively low testing rmse for the preliminary model we developed in part I, the model actually has an overfitting problem. The model would produce high rmse for the training data set and the diagnosis plots also indicate that the model fits the test data set too closely as all the residuals are close to zero. Therefore, as we were developing our final model, we wanted to balance the performances of our models on the training data set and the testing data set. Although the accuracy of our predictions for test data set is important, we still want to avoid overfitting problem. We started with a full model with all the available predictors selected with some modification, filtering predictors for redundancy, multicollinearity and adding some factor variables indicating if a painting has certain features. For redundant predictors such as "mat", "material" and "materialCat", since they all contain the same information, we only kept "mat" and dropped the other two predictors. For predictors with high multicollinearity with each other, we filtered those less important variables. For example, for "Height_in", "Width_in" and "Surface_Rect", we kept "Surface_Rect" to represent the surface area of the paintings and dropped the other two predictors. We also added new predictors such as "SurfaceNA" to indicate if the surface of a painting is specifies at the auction. Besides these modifications of the training data set, we also needed to handle the predictors having new levels in the testing and validation data sets. For such predictors, since usually the number of occurrences of their elements with new levels are small, for most of the times, we included them to an already existing or newly created category: "other". As mentioned in Part I, we also converted the data types of predictors to appropriate types and perform some transformations on predictors.

 * Predictor Selection  
 
After we have completed our data set modification, we tried various models with all the predictors we have: LASSO, BMA, random forest and BART. Among these models, only LASSO would preform some sort of free variable selection and therefore control the number of predictors while as indicated by the lectures: "BART is similar to Boosting in that the mean function is a sum of trees, but uses a Bayesian approach to control complexity. Trees can be of different sizes and the number of trees can be large without overfitting". Therefore, in order to control the previous overfitting problems, we want to perform some varaible selection based on AIC, BIC or other criteria.  

Under AIC(forward), we have the following predictors:  
position,dealer , year , diff_origin ,origin_author, origin_cat, endbuyer , 
    type_intermed , Surface , nfigures , engraved , prevcoll , artistliving,original,othartist,
    paired , finished , lrgfont,  othgenre , portrait , still_life , figures,relig,landsALL,
    discauth , school_pntg_recode , SurfaceNA,lands_sc, singlefig, history,allegory, pastorale,
    other, mat_recode, shape_recode  
    
Under BMA(posterior probability larger than 0.5), we have the following predictors:  
dealer , year , origin_author , origin_cat ,
                  diff_origin , endbuyer , type_intermed , Surface ,
                  nfigures , engraved , prevcoll , paired , finished ,
                  lrgfont , othgenre , portrait , still_life , discauth ,
                  school_pntg_recode , SurfaceNA , shape_recode  
                  
After comparing the training(calculated locally) and testing RMSE(from the leaderboard) for each model mentioned above with full predictors, predictors under AIC and predictors under BIC, we find that the predictors selected  under AIC has the best overall performance.

 * Model Selection  
 
 After we selected the predictors, we would like to compare the models, we used the training and testing rmse summarized in the following table 1:
 
```{r}
#random forest
library(randomForest)
rf.nes = randomForest(logprice ~ dealer + year + origin_author + origin_cat +
                  diff_origin + endbuyer + type_intermed + Surface +
                  nfigures + engraved + prevcoll + paired + finished +
                  lrgfont + othgenre + portrait + still_life + discauth +
                  school_pntg_recode + SurfaceNA + shape_recode, data=train, mtry=4,  importance =TRUE) 

yhat.rf.test= predict(rf.nes ,newdata =test)

yhat.rf.train= predict(rf.nes)

train.rmse.RF = rmse(exp(as.numeric(paintings_train$logprice)), exp(as.numeric(yhat.rf.train)))

#test rmse
#1423.688
```


```{r}
library(glmnet)
###lasso model
##training

#convert the training set into model matrix
x = model.matrix(logprice~dealer:year + origin_cat + diff_origin + 
    artistliving + endbuyer + type_intermed + Surface + nfigures + 
    engraved + prevcoll + othartist + paired + finished +lrgfont + 
    landsALL + lands_sc + othgenre + portrait + still_life + 
    discauth + mat_recode + school_pntg_recode:year + SurfaceNA + 
    shape_recode
, train)

#extract y
y= train$logprice

#define grid
grid = 10^(seq(10,-2, length=100))

#initial model
lasso.mod= glmnet(x,y,alpha=1, lambda=grid)
set.seed(1)
cv.out = cv.glmnet(x,y, alpha=1)

#extract best lambda
bestlam=cv.out$lambda.min

#lasso with best lambda
lasso.mod.best = glmnet(x,y,alpha=1, lambda=bestlam)

#prediction for training dataset
pred.train.lasso=predict(lasso.mod.best,x)

#calculate the training rmse for LASSO
train.rmse.lasso=rmse(exp(train$logprice),exp(pred.train.lasso))


##testing
#convert the test data set to model matrix
test=test%>%
  mutate(logprice=1)
x.test = model.matrix(logprice~dealer:year + origin_cat + diff_origin + 
    artistliving + endbuyer + type_intermed + Surface + nfigures + 
    engraved + prevcoll + othartist + paired + finished +lrgfont + 
    landsALL + lands_sc + othgenre + portrait + still_life + 
    discauth + mat_recode + school_pntg_recode:year + SurfaceNA + 
    shape_recode
,test)

#prediction for test data using the lasso model with best lambda
pred.test=predict(lasso.mod.best, x.test)

#exponentialte to get Price from log(Price)
pred.test=exp(pred.test)


#lasso model test rmse
#1677.637
```


```{r}
library(BAS)
#BMA

#using Bayesian Model Averaging with g-prior
df.bas = bas.lm(logprice~dealer + year + origin_author + origin_cat +
                  diff_origin + endbuyer + type_intermed + Surface +
                  nfigures + engraved + prevcoll + paired + finished +
                  lrgfont + othgenre + portrait + still_life + discauth +
                  school_pntg_recode + SurfaceNA + shape_recode, data=train,
prior="g-prior", a=nrow(train), modelprior=uniform(),
method="MCMC", MCMC.iterations = 200000, thin = 20)

#achieve prediction got testing set
y.test.bma = predict(df.bas, newdata = test, estimator = "BMA", se.fit = TRUE, prediction = FALSE)

#achieve prediction got trainiing data
y.train.bma = predict(df.bas,  estimator = "BMA", se.fit = TRUE, prediction = FALSE)

#calculate training rmse for BMA
train.rmse.BMA = rmse(exp(as.numeric(paintings_train$logprice)), exp(as.numeric(y.train.bma$fit)))

#test rmse
#1529.7337
```


```{r}
library(dplyr)
library(BayesTree)

#BART

#select predictors from the cleaned train dataset
x.train.new=train%>%
  dplyr::select(position,dealer , year , diff_origin ,origin_author, origin_cat, endbuyer , 
    type_intermed , Surface , nfigures , engraved , prevcoll , artistliving,original,othartist,
    paired , finished , lrgfont,  othgenre , portrait , still_life , figures,relig,landsALL,
    discauth , school_pntg_recode , SurfaceNA,lands_sc, singlefig, history,allegory, pastorale,
    other, mat_recode, shape_recode
)

#select y for fitting BART model 
y.train.new=train$logprice

#select predictors from the cleaned test dataset
x.test.new=test%>%
 dplyr:: select(position,dealer , year , diff_origin ,origin_author, origin_cat, endbuyer , 
    type_intermed , Surface , nfigures , engraved , prevcoll , artistliving,original,othartist,
    paired , finished , lrgfont,  othgenre , portrait , still_life , figures,relig,landsALL,
    discauth , school_pntg_recode , SurfaceNA,lands_sc, singlefig, history,allegory, pastorale,
    other, mat_recode, shape_recode

)

#fit the model
set.seed(1)
bart.nes.test=bart(x.train=x.train.new,y.train=y.train.new,x.test=x.test.new,verbose=FALSE)
set.seed(1)
bart.nes.train=bart(x.train=x.train.new,y.train=y.train.new,x.test=x.train.new,verbose=FALSE)

#achieve the predicted values for log(price)
pihat.bart.test=apply(bart.nes.test$yhat.test,2,mean)
pihat.bart.train=apply(bart.nes.train$yhat.test,2,mean)

#achieve the predicted values for price by exponentiating log(price)
y.test=exp(pihat.bart.test)
y.train=exp(pihat.bart.train)

#calculate the training rmse for BART
train.rmse.BART=rmse(exp(train$logprice),y.train)
```

```{r, echo=FALSE}
table=cbind(c(train.rmse.lasso,train.rmse.BMA,train.rmse.BART,train.rmse.RF),c(1677.637,1527.9337,1423.688,1487.458))
colnames(table)=c("Training RMSE","Testing RMSE")
rownames(table)=c("LASSO","BMA","BART","Random Forest")
kable(table)
```

Based on the table(testing RMSEs are form the leaderboard), among the models, BART and Random Forest has relatively low testing RMSE below 1500. However, Random Forest is possibly overfitting the test data with relatively high training RMSE while BART has the lowest training RMSE. Moreover, BART performs better than LASSO and BMA in both training and testing RMSE. Thus, we selected BART with the predictors selected previously as our final model. 


 * Residual Plot and Discussion
 
```{r}
#residual plot for BART
residual = scale(pihat.bart.train - train$logprice)
ggplot(data.frame(cbind(residual,  train$logprice)), aes(x= pihat.bart.train, y=as.numeric(residual))) +
    geom_point(shape=1)  +
    ggtitle("Residual Plot for BART Regression") +
    labs(x="Fitted Value for log(Price)",y="Standardize Residual") 

#BART test rmse
#1487.458
```

Discussion:from the Residuals vs. Fitted diagram, we can see no distinct pattern indicating non-linear relationship. Residuals are generally equally distributed around a horizontal line. Therefore, the linear relationship assumption appears to be met.


 * How Prediction Intervals Obtained  
 
Note that for the confidence interval, we used "bartMachine". Since "BayesTree" package has a built-in predict function, since it's uses bootstrap method and running long iterations is painfully slow. But small sample size could be inappropirate to construct a prediction interval. Hence we uses "bartMachine" which parallelize its calculation and runs much faster.


#Assessment of the final model


* Model evaluation: 

```{r}
par(mfrow = c(1,1))
plot(bart.nes.train)
pd.election=pdbart(x.train=train[,-7], y.train=train[,7], xind= 10, verbose=FALSE)
```


From the first plot, it shows the uncertanty of the predictions, the uncertainty around the bounday of y(ie. 0 and 10) is larger than the one in the middle.
Since `pdbart` function is computationally intensive, we only plot one continious variable "Surface"; For its partial dependence plot, it shows parital marginal contribution of "Surface", since the plot doesn't include 0, and the dependence increase the size of surface increase, it shows that this predictors contribute a lot.

* Model testing

As mentioned above, we have tested other models as well, even though Bart lack of interpretation ability, its combine advantage of boosting(decrease residual of previous tree by ) and can increase size of the tree without overfitting. As the similar case with LASSO and random forest, it is hard to obtain confidence interval, although the prediction come with the form of matrix, with 1131 observation and 1000 instances for each observation, if we use confint with c(2.5%, 97.5%), the coverage show us under fit, so we decide to use `BartMachine` instead as suggest in the lecture. Granted, there is no variable selection within Bart process and it provide no interpretation of varible in the end, with the lower RMSE across the board, we finally choose Bart after all.


* Model result

The results for the top 10 valued  paintings in the validation data are shown below:

```{r}
load("paintings_validation.Rdata")


validation = clean.new(paintings_validation)%>% select(-lot)

x.test.valid=validation%>%
  dplyr::select(position,dealer , year , diff_origin ,origin_author, origin_cat, endbuyer , 
    type_intermed , Surface , nfigures , engraved , prevcoll , artistliving,original,othartist,
    paired , finished , lrgfont,  othgenre , portrait , still_life , figures,relig,landsALL,
    discauth , school_pntg_recode , SurfaceNA,lands_sc, singlefig, history,allegory, pastorale,
    other, mat_recode, shape_recode

)
set.seed(1)

bart.nes.valid=bart(x.train=x.train.new,y.train=y.train.new,x.test=x.test.valid,verbose=FALSE)

pihat.bart.valid=apply(bart.nes.valid$yhat.test,2,mean)


paintings_validation$price_pred = exp(pihat.bart.valid)

part_valid = paintings_validation %>% select(dealer, endbuyer,lrgfont, price_pred) %>% 
  arrange(desc(price_pred)) %>% .[1:10,]

kable(part_valid)
```

As the result shows, all top 10 price painting are involved with Dealer R, and the paintings are end up with collecter and dealer, futhemore, dealer devote an additional paragraph when introduce the paintings; all these results show consistency with our assumption in partI EDA anlysis.


#Conclusion  

Summary of Results:  

The performances of the models in Part I and Part II on the testing and training data sets are summarized in the following table.
```{r}
##test rmse 1272.9
##train rmse 2145.83
lm.results=c(2145.83,1272.9)
table=rbind(table,lm.results)
rownames(table)[5]="OLS"
kable(table)
```

As we can see from the table above, for our OLS model in Part I, although we achieved the lowest testing RMSE, we found that the OLS model has overfitted the test data set and therefore the resulting training RMSE is extremly high. For the OLS model, in the summary, we have R-squared equal to 0.5899, which suggests that 58.99% of the variation has been explained by our predictors.  

In Part II, we tried to resolve our problem by balancing the performance of our models on testing and training data sets. We tried LASSO, BMA, Random Forest and BART models with predictors selected by AIC, BIC and posterior probability in BMA. It turns out that the BART model with the predictors selected under AIC gave us the overall lowest and training and testing RMSE as well as relative balanced performance over testing and training data sets. 


Things learned:

 * The first thing we learned is data cleaning. It costed most of our time during first part of our project. Most datasets we will see in real life could be somewhat messy and need a lot of preprocessing before we actually start modeling. Since our train, test, validation sets have the same structure. We coded a function to clean up our dataset to streamline this process. In addition, some categorical variables in test/validation sets have more levels, which we needed to account for and update our cleaning process.
 
 * Secondly, we learned the importance of not overfitting test data. During the first part, we tried hard to get on top ranks on the leaderboard and ended up with test RMSE of around 1270 but train RMSE with more than 2000. It's clear that we overfitted the test data and we do not expect such a model will perform well on the validation data. Later we tried other models such as LASSO, Bayesian Model Averaging, trees, random forest, boosting, extreme gradient boosting and Bayesian Additive Regression Tree. Some models run into the issue of overfitting part of the dataset. We could examine this by performing cross validation to see which model has the most balanced RMSE for train and test data. However, some models are more robust for overfitting, such as extreme gradient boosting and BART. XGBoost has a built in parameter(gamma) to control overfitting. And as mentioned in lecture, BART handles overfitting very well, and as for our datasets, BART has the most balanced RMSE(and it's low).

 * Last, we learned that compuation time could be a huge problem. Since the dataset have more than 40 variables, fitting models such as XGBoost is not as easy as fitting a linear model. Also, since tree/boosting methods do not have inherent confidence/prediction intervals, we need to take bootstrap samples and it could take a long time. At last we learned a package "bartMachine" which uses Java for parallel processing.
 
 * If we have more time: we would explore more with bartMachine and add a cross-validation method, which could improve our results and make our model more robust. Unfortunately, our laptops have limited RAM and we might need to distribute the compuation on cloud service, such as AWS. In addtion, we did not use author as a predictor but intuitively, it could be an important predictor. Since author has too many levels, we could recode this variables according to how celebrated the author was.
